{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsxOPfRMaTp3Km1TpOuS/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihatestudysomuch/pytorch/blob/main/Pytorch_5%EC%9E%A5_%EC%B5%9C%EC%A0%81%ED%99%94_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98_%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 경사하강법(gradient descent algorithm) - reiview loss func\n",
        "\n",
        "\n",
        "1.   손실함수는 오차제곱의 평균값을 나타내기 때문에 손실함수가 최소라는 것은 실제 정답과 오차간의 차이가 최소가 되어, 미지의 데이터에 대한 결과 예츠깅 더 잘 이루어지는 것을 의미한다.\n",
        "2.   앞서 말했듯, 손실함수는 가중치 W, 바이어스 b에 영향을 받기 때문에 손실함수가 최소가 되는 W, b를 찾는 것이 최종 목표이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "bfb7zVu0K9Y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사하강법(gradient descent algorithm) - 원리\n",
        "\n",
        "\n",
        "1.   임의의 가중치 W 선택\n",
        "2.   그 W에서 직선의 기울기를 나타내는 미분값 구함\n",
        "\n",
        "1.   미분값이 작아지는 방향으로 W 감소\n",
        "2.   최종적으로 기울기가 작아지지 않는 곳이 손실함수가 최소일 때, W값\n",
        "\n",
        "\n",
        "---\n",
        "위 방법처럼 W에서 직선의 기울기인 미분값을 구하고 미분값이 작아지는 방향으로 진행하여 손실함수의 최소값을 찾는 방법을 경사하강법(gradient descent alogrithm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4dHYEhZlPKyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사하강법(gradient descent algorithm) - W값 구하기\n",
        "\n",
        "\n",
        "1.   임의의 W에 대하여 E(W,b) 그래프(최고 차항이 양수인 2차함수로 가정)에서 미분값(기울기)가 양수(+)라면 W를 왼쪽으로 이동(W값을 줄여야함)\n",
        "2.   임의의 W에 대하여 E(W,b) 그래프(최고 차항이 양수인 2차함수로 가정)에서 미분값(기울기)가 음수(-)라면 W를 오른족으로 이동(W값을 키워야함)\n",
        "\n",
        "1.   결론적으로 W = W - lr * 미분값 (*여기서 lr은 learning rate로 W값의 증/감 비율을 나타냄)으로 변한다.\n",
        "\n",
        "1.   바이어스 b도 같은 맥락으로 손실함수의 최소일 때, b값을 찾는다.\n",
        "\n",
        "---\n",
        "## 최적의 계산 프로세스\n",
        "\n",
        "\n",
        "1.   traning data input\n",
        "\n",
        "1.   learning(손실함수 계산)\n",
        "2.   손실함수값이 최소인지 확인 후 아니라면 W, b값을 수정하고 다시 learning\n",
        "\n",
        "\n",
        "2.   손실함수값이 최소인지 확인되면 learning 종료\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hKlLwTlfRCDI"
      }
    }
  ]
}