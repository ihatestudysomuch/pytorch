{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2OOHvc7b3oPgHiLQlvghN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihatestudysomuch/pytorch/blob/main/Pytorch_27%EC%9E%A5_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU 구조\n",
        "1. GRU는 LSTM과 비슷하지만, 더 간단한 구조로 이루어져 있어서 계산의 이점이 있다.\n",
        "  - cell state대신 GRU의 출력 H값이 그 것을 대신한다.\n",
        "  - LSTM에 비해서 활성 함수(sigmoid, tanh)의 연산 횟수가 적기 때문에(5 -> 3) 연산량을 줄일 수 있다.\n",
        "2. 각 구조의 기능\n",
        "  - reset gate의 Rt는 과거의 정볼르 얼마나 잊을지(또는 기억할지)정하는 gate로 sigmoid 함수를 사용한다.\n",
        "  - update gate의 Ut는 과거와 현재 정보 중에서 어떤 정보를 더 많이 업데이트 할지 결정하는 gate로 출력값 Ut는 현시점에서 가져가야 할 데이터 양을 말하고 1- Ut는 잊어버려야 할 데이터 양을 말하며 tanh 함수를 사용한다.. LSTM의 input + forget gate의 개념이다.\n",
        "  - candidate 단계는 다음 시점으로 전달해줄 데이터 Ht를 만들기 위해, 현재 시점의 데이터를 선정하는 단계로 tanh 함수를 사용한다.  \n",
        "  * 잊었을 수 있지만 sigmoid: 0 ~ 1, tanh: -1 ~ 1\n",
        "  - 출력값 Ht는 이전 단계의 출력값 Ht-1에서 얼마나 잊을지, 현 시점 데이터에서 얼마만큼 다음 단계로 가져갈지 계산한 값이다."
      ],
      "metadata": {
        "id": "K-B5TR2se11B"
      }
    }
  ]
}